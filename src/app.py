import streamlit as st
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from datetime import datetime, timedelta
import requests
import praw

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="Sentiment Compass: Ising Model Edition",
    page_icon="ğŸ§­",
    layout="wide"
)

# --- åˆå§‹åŒ– VADER åˆ†æå™¨ (æ¯” TextBlob æ›´æ•æ„Ÿ) ---
@st.cache_resource
def get_vader_analyzer():
    return SentimentIntensityAnalyzer()

vader = get_vader_analyzer()

# --- æ ¸å¿ƒç‰©ç†é€»è¾‘å‡½æ•° (Physics Core) ---

def analyze_signal(text):
    """
    æ··åˆåˆ†ææ¨¡å‹ï¼š
    1. VADER Compound: æ•æ‰å¾®å¼±æƒ…ç»ªä¿¡å· (-1 åˆ° 1)ï¼Œä½œä¸ºè‡ªæ—‹ (Spin)ã€‚
    2. TextBlob Subjectivity: æ•æ‰ä¸»è§‚ç¨‹åº¦ (0 åˆ° 1)ï¼Œä½œä¸ºå™ªå£°/æ¸©åº¦ (Temperature)ã€‚
    """
    if not text:
        return 0.0, 0.0
    
    # VADER å¤„ç†ç¤¾äº¤æ–‡æœ¬/å™ªå£°èƒ½åŠ›æ›´å¼º
    vs = vader.polarity_scores(text)
    spin = vs['compound'] 
    
    # TextBlob å¤„ç†ä¸»è§‚æ€§
    blob = TextBlob(text)
    noise_level = blob.sentiment.subjectivity
    
    return spin, noise_level

def calculate_ising_metrics(df):
    """
    è®¡ç®—ä¼Šè¾›æ¨¡å‹å…³é”®æŒ‡æ ‡ã€‚
    è¾“å…¥ df éœ€åŒ…å« 'Sentiment' åˆ— (å³ Spin)ã€‚
    """
    if df.empty:
        return 0, 0
    
    spins = df['Sentiment'].values
    
    # 1. ç£åŒ–å¼ºåº¦ (Magnetization, M): èˆ†è®ºçš„ä¸€è‡´æ€§æ–¹å‘
    # M ~ <s_i>
    magnetization = np.mean(spins)
    
    # 2. ç£åŒ–ç‡ (Susceptibility, Ï‡): èˆ†è®ºçš„â€œæ˜“æ„Ÿæ€§â€æˆ–â€œè„†å¼±åº¦â€
    # åœ¨ç‰©ç†ä¸Šï¼ŒÏ‡ = ( <M^2> - <M>^2 ) / T
    # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–ä¸ºè‡ªæ—‹çš„æ–¹å·® (Variance of Sentiment)
    # ç‰©ç†æ„ä¹‰ï¼šå½“ Ï‡ å˜å¤§æ—¶ï¼Œç³»ç»Ÿå¤„äºä¸´ç•Œæ€ï¼Œå¾®å°çš„å¤–éƒ¨æ–°é—»(å¤–åœº H)å°±èƒ½å¼•å‘å´©å¡Œ(ç›¸å˜)ã€‚
    susceptibility = np.var(spins)
    
    return magnetization, susceptibility

# --- æ•°æ®è·å–å‡½æ•° ---

def fetch_newsapi_data(topic, api_key):
    """ä» NewsAPI è·å–å¹¿æ’­å‹æ•°æ® (Broadcasting)"""
    url = f"https://newsapi.org/v2/everything?q={topic}&language=en&sortBy=publishedAt&pageSize=100&apiKey={api_key}"
    try:
        response = requests.get(url)
        data = response.json()
        if data.get("status") != "ok":
            st.error(f"NewsAPI Error: {data.get('message')}")
            return pd.DataFrame()
        
        articles = data.get("articles", [])
        processed_data = []
        for article in articles:
            text = f"{article.get('title', '')}. {article.get('description', '')}"
            spin, noise = analyze_signal(text)
            processed_data.append({
                "Date": article.get("publishedAt", "")[:19], # ISO format
                "Text": text,
                "Sentiment": spin,     # Spin
                "Subjectivity": noise, # Temperature
                "Source": "NewsAPI"
            })
        return pd.DataFrame(processed_data)
    except Exception as e:
        st.error(f"NewsAPI è¯·æ±‚å¤±è´¥: {e}")
        return pd.DataFrame()

def fetch_reddit_data(topic, client_id, client_secret, user_agent="sentiment_compass_v2"):
    """ä» Reddit è·å–å™ªå£°å‹æ•°æ® (Noise/Micro-states)"""
    try:
        reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)
        subreddit = reddit.subreddit("all")
        # æœç´¢ç›¸å…³è®¨è®ºï¼Œä¸å±€é™äºé‡‘èæ¿å—
        posts = subreddit.search(topic, sort='new', limit=100)
        
        processed_data = []
        for post in posts:
            # ç»“åˆæ ‡é¢˜å’Œé«˜èµè¯„è®ºå¯èƒ½æ›´ä½³ï¼Œè¿™é‡Œå…ˆåªå–æ ‡é¢˜ä»¥ä¿è¯é€Ÿåº¦
            text = f"{post.title} {post.selftext[:200]}" 
            spin, noise = analyze_signal(text)
            processed_data.append({
                "Date": datetime.fromtimestamp(post.created_utc).isoformat(),
                "Text": text,
                "Sentiment": spin,
                "Subjectivity": noise,
                "Source": "Reddit"
            })
        return pd.DataFrame(processed_data)
    except Exception as e:
        st.error(f"Reddit API é”™è¯¯ (è¯·æ£€æŸ¥ Credentials): {e}")
        return pd.DataFrame()

def generate_simulation_data():
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„ç›¸å˜æ•°æ® (Monte Carlo Simulation ä¼ªé€ )ã€‚
    æ¨¡æ‹Ÿä¸€ä¸ªç³»ç»Ÿä»æ— åº(Disordered) -> ä¸´ç•Œæ€(Critical) -> æœ‰åº(Ordered) çš„è¿‡ç¨‹ã€‚
    """
    dates = pd.date_range(end=datetime.now(), periods=100, freq='H')
    data = []
    
    # é˜¶æ®µ 1: éšæœºå™ªå£° (æ— åº)
    for i in range(40):
        spin = np.random.normal(0, 0.2) # å‡å€¼0ï¼Œæ–¹å·®å°
        data.append({"Date": dates[i], "Sentiment": spin, "Subjectivity": np.random.random()})
        
    # é˜¶æ®µ 2: ä¸´ç•Œæ¶¨è½ (Critical Fluctuations) - å‡å€¼ä»ä¸º0ï¼Œä½†æ–¹å·®å‰§å¢
    for i in range(40, 70):
        # æ¨¡æ‹Ÿæ„è§åˆ†æ­§å·¨å¤§ï¼šæœ‰äººæåº¦çœ‹å¤šï¼Œæœ‰äººæåº¦çœ‹ç©º
        spin = np.random.choice([0.8, -0.8]) + np.random.normal(0, 0.1) 
        data.append({"Date": dates[i], "Sentiment": spin, "Subjectivity": 0.9})
        
    # é˜¶æ®µ 3: ç›¸å˜/å¯¹ç§°æ€§ç ´ç¼º (Symmetry Breaking) - åç¼©åˆ°ä¸€ä¸ªæ–¹å‘
    for i in range(70, 100):
        spin = -0.9 + np.random.normal(0, 0.1) # çªç„¶å´©ç›˜
        data.append({"Date": dates[i], "Sentiment": spin, "Subjectivity": 0.6})
        
    return pd.DataFrame(data)

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("âš™ï¸ æ¢æµ‹å™¨è®¾ç½®")
    st.markdown("### 1. é€‰æ‹©æ•°æ®æº")
    data_source = st.selectbox(
        "Data Source", 
        ["Simulation (ç‰©ç†æ¼”ç¤º)", "NewsAPI (å¹¿æ’­ä¿¡å·)", "Reddit (å¾®è§‚å™ªå£°)"]
    )
    
    api_key = None
    reddit_cid = None
    reddit_sec = None
    
    if data_source == "NewsAPI (å¹¿æ’­ä¿¡å·)":
        api_key = st.text_input("NewsAPI Key", type="password")
    elif data_source == "Reddit (å¾®è§‚å™ªå£°)":
        st.info("éœ€è¦ Reddit App Credentials")
        reddit_cid = st.text_input("Client ID", type="password")
        reddit_sec = st.text_input("Client Secret", type="password")

    st.divider()
    st.markdown("### 2. ç‰©ç†å‚æ•°è§£é‡Š")
    st.markdown("**Magnetization ($M$):**\nå¹³å‡èˆ†è®ºæ–¹å‘ã€‚$M \\approx 0$ è¡¨ç¤ºå¤šç©ºå¹³è¡¡ã€‚")
    st.markdown("**Susceptibility ($\chi$):**\nèˆ†è®ºæ˜“æ„Ÿæ€§(æ–¹å·®)ã€‚\nâš ï¸ **é«˜ $\chi$ + ä½ $M$ = æš´é£é›¨å‰çš„å®é™ã€‚**")

# --- ä¸»é¡µé¢ ---
st.title("ğŸ§­ Sentiment Compass: Ising Model Edition")
st.caption("åŸºäºç»Ÿè®¡ç‰©ç† (Statistical Physics) çš„èˆ†è®ºç›¸å˜æ¢æµ‹å™¨")

tab1, tab2 = st.tabs(["ğŸ”¬ å¾®è§‚ç²’å­åˆ†æ (Micro)", "ğŸ“ˆ å®è§‚ç›¸å˜ç›‘æ§ (Macro)"])

# --- Tab 1: å•æ–‡æœ¬åˆ†æ ---
with tab1:
    st.subheader("å•æ¡è®¯æ¯è‡ªæ—‹æµ‹å®š")
    txt = st.text_area("è¾“å…¥æ–‡æœ¬ (æ¨æ–‡/è¯„è®º/æ–°é—»):", value="I am worried about the inflation, but AI stocks seem unstoppable!")
    if st.button("è®¡ç®—è‡ªæ—‹ (Calculate Spin)"):
        spin, noise = analyze_signal(txt)
        
        c1, c2, c3 = st.columns(3)
        c1.metric("è‡ªæ—‹æ–¹å‘ (Spin)", f"{spin:.2f}")
        c2.metric("çƒ­å™ªå£° (Temp)", f"{noise:.2f}")
        
        state = "ä¸­æ€§ (Paramagnetic)"
        if spin > 0.3: state = "ä¸Šè‡ªæ—‹ (Positive) ğŸŸ¢"
        if spin < -0.3: state = "ä¸‹è‡ªæ—‹ (Negative) ğŸ”´"
        c3.metric("å½“å‰çŠ¶æ€", state)

# --- Tab 2: å®è§‚åˆ†æ ---
with tab2:
    st.subheader("èˆ†è®ºåœºç›¸å˜ç›‘æ§ (Phase Transition Monitor)")
    
    topic = st.text_input("è¾“å…¥è¯é¢˜ (Topic):", value="Quantum Computing")
    start = st.button("å¯åŠ¨æ¢æµ‹ (Initialize Detector)")
    
    if start:
        df = pd.DataFrame()
        
        with st.spinner("æ­£åœ¨é‡‡é›†åœºæ•°æ®..."):
            if data_source == "Simulation (ç‰©ç†æ¼”ç¤º)":
                df = generate_simulation_data()
                st.warning("âš ï¸ å½“å‰ä¸ºæ¨¡æ‹Ÿæ•°æ®ï¼šå±•ç¤ºäº†å…¸å‹çš„ä»æ— åºåˆ°ç›¸å˜çš„è¿‡ç¨‹ã€‚")
            elif data_source == "NewsAPI (å¹¿æ’­ä¿¡å·)":
                if api_key:
                    df = fetch_newsapi_data(topic, api_key)
                else:
                    st.error("è¯·è¾“å…¥ NewsAPI Key")
            elif data_source == "Reddit (å¾®è§‚å™ªå£°)":
                if reddit_cid and reddit_sec:
                    df = fetch_reddit_data(topic, reddit_cid, reddit_sec)
                else:
                    st.error("è¯·è¾“å…¥ Reddit Credentials")

        if not df.empty:
            # æ•°æ®é¢„å¤„ç†
            df['Date'] = pd.to_datetime(df['Date'])
            df = df.sort_values('Date')
            
            # æ»‘åŠ¨çª—å£è®¡ç®—ç‰©ç†æŒ‡æ ‡ (æ¨¡æ‹Ÿéšæ—¶é—´çš„æ¼”åŒ–)
            # æˆ‘ä»¬ä½¿ç”¨ Expanding window æ¥æ¨¡æ‹Ÿä¿¡æ¯çš„ç´¯ç§¯ï¼Œæˆ–è€… Rolling window æ¨¡æ‹Ÿç¬æ—¶çŠ¶æ€
            window_size = len(df) // 10 if len(df) > 20 else 5
            
            df['Magnetization'] = df['Sentiment'].rolling(window=window_size).mean()
            df['Susceptibility'] = df['Sentiment'].rolling(window=window_size).var()
            
            # å¸ƒå±€ï¼šä¸Šå›¾ä¸ºåŸå§‹è‡ªæ—‹åˆ†å¸ƒï¼Œä¸‹å›¾ä¸ºç‰©ç†æŒ‡æ ‡
            
            # å›¾1: è‡ªæ—‹åˆ†å¸ƒæ•£ç‚¹å›¾
            fig_scatter = px.scatter(
                df, x='Date', y='Sentiment', color='Subjectivity',
                title=f"å¾®è§‚è‡ªæ—‹åˆ†å¸ƒ (Micro-States Scatters) - {topic}",
                color_continuous_scale='Bluered',
                range_y=[-1.1, 1.1]
            )
            st.plotly_chart(fig_scatter, use_container_width=True)
            
            # å›¾2: åºå‚é‡ (M) ä¸ å“åº”å‡½æ•° (Ï‡)
            st.markdown("### ğŸ“Š åºå‚é‡ä¸ä¸´ç•ŒæŒ‡æ ‡")
            
            # åŒè½´å›¾
            fig_macro = go.Figure()
            
            # å·¦è½´ï¼šç£åŒ–å¼ºåº¦ M
            fig_macro.add_trace(go.Scatter(
                x=df['Date'], y=df['Magnetization'],
                name='ç£åŒ–å¼ºåº¦ (Avg Sentiment)',
                line=dict(color='blue', width=2)
            ))
            
            # å³è½´ï¼šç£åŒ–ç‡ Ï‡
            fig_macro.add_trace(go.Scatter(
                x=df['Date'], y=df['Susceptibility'],
                name='ç£åŒ–ç‡/æ˜“æ„Ÿæ€§ (Variance)',
                line=dict(color='red', width=2, dash='dot'),
                yaxis='y2'
            ))
            
            fig_macro.update_layout(
                title="ç›¸å˜å‰å…†ç›‘æ§ (Order Parameter vs Susceptibility)",
                yaxis=dict(title="Magnetization (M)", range=[-1, 1]),
                yaxis2=dict(
                    title="Susceptibility (Ï‡)", 
                    overlaying='y', 
                    side='right',
                    range=[0, df['Susceptibility'].max() * 1.2]
                ),
                hovermode="x unified"
            )
            
            st.plotly_chart(fig_macro, use_container_width=True)
            
            # ç‰©ç†æ´å¯Ÿè§£é‡Š
            curr_sus = df['Susceptibility'].iloc[-1]
            st.info(f"""
            **ç‰©ç†è¯Šæ–­æŠ¥å‘Š:**
            - å½“å‰ç£åŒ–ç‡ ($\chi$): **{curr_sus:.4f}**
            - **è§£è¯»**: 
                - å¦‚æœ $\chi$ è¾ƒä½ä¸” $M$ æ¥è¿‘ 0ï¼šç³»ç»Ÿå¤„äº**æ— åºç›¸ (Disordered Phase)**ï¼Œå™ªå£°ä¸ºä¸»ã€‚
                - å¦‚æœ $\chi$ æ€¥å‰§å‡é«˜ï¼šç³»ç»Ÿå¤„äº**ä¸´ç•Œç‚¹ (Critical Point)**ã€‚å³ä½¿ $M$ çœ‹èµ·æ¥æ­£å¸¸ï¼Œå¸‚åœºä¹Ÿæå…¶è„†å¼±ï¼Œéšæ—¶å¯èƒ½å‘ç”Ÿå¯¹ç§°æ€§ç ´ç¼ºã€‚
                - å¦‚æœ $M$ å¾ˆå¤§ä¸” $\chi$ å›è½ï¼šç³»ç»Ÿå·²å®Œæˆ**ç›¸å˜ (Phase Transition)**ï¼Œè¿›å…¥æœ‰åºç›¸ï¼ˆå•è¾¹è¡Œæƒ…ï¼‰ã€‚
            """)
            
            with st.expander("æŸ¥çœ‹åŸå§‹æ•°æ® (Raw Lattice Data)"):
                st.dataframe(df.sort_values(by='Date', ascending=False))